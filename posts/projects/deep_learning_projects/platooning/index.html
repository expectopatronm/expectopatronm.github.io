<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.68.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Platooning Demonstrator with Deep Learning and Computer Vision &middot; Shankar Kumar</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="/css/poole.css">
  <link type="text/css" rel="stylesheet" href="/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <img src="/myphoto.png">
    <div class="sidebar-about">
      <a href="/"><h1>Shankar Kumar</h1></a>
      <p class="lead">
       Applied Machine Learning Enthusiast. Interested in Data Science, Autonomous Systems, Computer Vision and Embedded Intelligence 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="/">Home</a> </li>
        <li><a href="/posts/about_me/"> About Me </a></li><li><a href="/posts/work_experience/"> Work Experience </a></li><li><a href="/posts/intro_projects/"> Projects </a></li><li><a href="/posts/photography/"> Photography </a></li>
      </ul>
    </nav>

    <p>&copy; 2020. All rights reserved. </p>
  </div>
</aside>



    <main class="content container">
    <div class="post">
  <h1>Platooning Demonstrator with Deep Learning and Computer Vision</h1>
  <time datetime=2020-02-29T00:00:00Z class="post-date">Sat, Feb 29, 2020</time>
  <p>Originally envisioned as a demonstrator for the Bosch AI CON 2019, the <em>platooning</em> system consists of two cars, a <em>leading car</em> and a <em>following car</em>. The leading car can be driven manually using a PS4 controller and the following car will autonomously follow the leading car.</p>
<p><img src="/images/inaction.gif" alt=""></p>
<p>In addition to this, the system currently is also capable of <em>Object Tracking</em>, <em>Velocity Estimation by Optical Flow Visual Odometry</em> and <em>Monocular Depth Estimation</em>.</p>
<hr>
<h2 id="abstract"><strong>Abstract</strong></h2>
<p>A sample output of what the following car <em>sees</em> in its camera stream is shown below.</p>
<p><img src="/images/sample_output.gif" alt=""></p>
<hr>
<h2 id="object-detection"><strong>Object Detection</strong></h2>
<p>The following car utilizes an <em>Object Detection DNN</em>, amongst other things, to identify and localize the leading car in its input camera stream with a <em>bounding box</em>. The object detection architecture is an <em>Inception V2</em> Convolutional Neural Network with a <em>Single Shot Detector (SSD)</em> for the actual object detection.</p>
<p><img src="/images/SSD_arch.png" alt=""></p>
<p>Information about the SSD can be found at <a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a>. The VGG-16 part is replaced by the Inception CNN.</p>
<p><img src="/images/inception_v2.png" alt=""></p>
<p>More details about the Inception V2 architecture can be found at <a href="https://arxiv.org/abs/1512.00567">https://arxiv.org/abs/1512.00567</a>. The Inception architecture was chosen for its good trade-off between speed and performance when compared to other models that are optimized to run on embedded systems. The comparison can be found at <a href="https://arxiv.org/abs/1611.10012">https://arxiv.org/abs/1611.10012</a>.</p>
<p>*Transfer Learning* was used in order to repurpose a SSD Inception model trained on the COCO (Common Objects in COntext) dataset, modified to detect a single class of objects - the leading car. Let us call this _custom_ object detection. We use images such as the ones shown below with the leading car in different environments, labelled by a bounding box for the training.</p>
<p><img src="/images/training_images.JPG" alt=""></p>
<p>Tensorflow provides a pre-trained model of this architecture. For information on the custom object detection training routine, you can refer <a href="https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html">https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html</a>. The retraining of the model is done on a Google Colab notebook to make use of the free (!) Tesla GPU and 64 GB RAM. The model can be fully or partially retrained based on performance needs. If privacy or data security is a concern, the notebook can be run on a local machine.</p>
<p>The Inference of the model is accelerated by TensorRT, details of which can be found at <a href="https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html">https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html</a>.</p>
<hr>
<h2 id="object-tracking"><strong>Object Tracking</strong></h2>
<p>The detected leading car is assigned an ID in order to identify it as a member of a series of platooning cars. If the leading car steers off frame and then reappears, the rudimentry tracking algorithm is capable of identifying that the car that is back in frame is the car with the same ID. It uses mathematical concepts such as maximizing the IOU (Intersection Over Union) metrics between bounding boxes in neighboring frames.This is made possible by, amongst other things, a Kalman Filter. The implementation details can be found at <a href="https://arxiv.org/abs/1602.00763">https://arxiv.org/abs/1602.00763</a>.</p>
<p><img src="/images/tracking_pic.JPG" alt=""></p>
<hr>
<h2 id="velocity-estimation"><strong>Velocity Estimation</strong></h2>
<p>Optical flow methods utilize the differences in pixel intensities across consequent frames to calculate apparent motion.</p>
<p><img src="/images/gif2.gif" alt=""></p>
<p>This fundamental method is usually used to determine the flow of an object across a frame, but gave me an idea that it can also be used to used to calculate self velocity (velocity of the following car) by considering the car itself as the moving object. The function <em>calcOpticalFlowFarneback</em> from OpenCV that facilitates this is the based on the paper found at <a href="https://rd.springer.com/chapter/10.1007/3-540-45103-X_50">https://rd.springer.com/chapter/10.1007/3-540-45103-X_50</a>.</p>
<hr>
<h2 id="depth-perception"><strong>Depth Perception</strong></h2>
<p>This is an effort to be able to sense depth information (monocular) from a 2D camera. Implementation details can be found at <a href="https://arxiv.org/abs/1806.01260">https://arxiv.org/abs/1806.01260</a>.</p>
<p><img src="/images/gif3.gif" alt=""></p>
<p>A rudimentary use case of this for obstacle detection has been implemented. The depth perception window displays the observed depths of the environment in a grayscale map (0-255, normalized). A grayscale pixel value of 0 would indicate the object being further away and a grayscale value of 255 woud indicate an object being the closest. Based on this, a horizontal grid of so called <em>superpixels</em> (cool name, right?!) is defined. The averaged grayscale value within these superpixels is noted and based on this a decision is made as to whether an obstacle is detected or not. A predefined threshold turns the superpixel window red when an obstacle is detected.</p>
<hr>
<h2 id="working-principle"><strong>Working Principle</strong></h2>
<p>The area of the bounding box generated fromt the object detection determines the speed at which the following car keeps up with the leading car - the smaller the area, the further away the leading car is, requiring a larger acceleration and vice versa.</p>
<p><img src="/images/gif1.gif" alt=""></p>
<p>The following car is also designed to maintain a fixed safe distance to the leading car based on the bounding box area. The vertical separator divides the frame into two halves and the centroid of the bounding box and its position along the x axis determines the steering angle of the following car.</p>
<p>The camera system also includes a pan-tilt mechanism, which is tasked with orienting the center of the camera to the center of the bounding box which in turn is used to set the steering angle. This provides an additional degree of freedom to the following method and helps when there are relatively sharp turns made by the leading car.</p>

</div>


    </main>

    
      
    
  </body>
</html>
